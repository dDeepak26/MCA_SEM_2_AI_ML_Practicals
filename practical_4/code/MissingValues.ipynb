{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO6q9euzcBYX25m8psw6EPQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eU2AArA1-XCh","executionInfo":{"status":"ok","timestamp":1711445713295,"user_tz":-330,"elapsed":512,"user":{"displayName":"Ramakrishnan Iyer","userId":"01413788399740302629"}},"outputId":"72469912-1ba8-4846-a785-2c60bbfac61f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Item_Identifier               0.000000\n","Item_Weight                  17.165317\n","Item_Fat_Content              0.000000\n","Item_Visibility               0.000000\n","Item_Type                     0.000000\n","Item_MRP                      0.000000\n","Outlet_Identifier             0.000000\n","Outlet_Establishment_Year     0.000000\n","Outlet_Size                  28.276428\n","Outlet_Location_Type          0.000000\n","Outlet_Type                   0.000000\n","Item_Outlet_Sales             0.000000\n","dtype: float64\n","Index(['Item_Identifier', 'Item_Weight', 'Item_Fat_Content', 'Item_Visibility',\n","       'Item_Type', 'Item_MRP', 'Outlet_Identifier',\n","       'Outlet_Establishment_Year', 'Outlet_Size', 'Outlet_Location_Type',\n","       'Outlet_Type', 'Item_Outlet_Sales'],\n","      dtype='object')\n","['Item_Identifier', 'Item_Weight', 'Item_Fat_Content', 'Item_Visibility', 'Item_Type', 'Item_MRP', 'Outlet_Identifier', 'Outlet_Establishment_Year', 'Outlet_Location_Type', 'Outlet_Type', 'Item_Outlet_Sales']\n"]},{"output_type":"execute_result","data":{"text/plain":["Item_Identifier              0.0\n","Item_Weight                  0.0\n","Item_Fat_Content             0.0\n","Item_Visibility              0.0\n","Item_Type                    0.0\n","Item_MRP                     0.0\n","Outlet_Identifier            0.0\n","Outlet_Establishment_Year    0.0\n","Outlet_Location_Type         0.0\n","Outlet_Type                  0.0\n","Item_Outlet_Sales            0.0\n","dtype: float64"]},"metadata":{},"execution_count":27}],"source":["import pandas as pd\n","import numpy as np\n","import csv\n","\n","# read the data\n","train=pd.read_csv(\"Train_UWu5bXk.csv\")\n","\n","# Note: The path of the file should be added while reading the data.\n","\n","# Now, we will check the percentage of missing values in each variable. We can use .isnull().sum() to calculate this.\n","\n","train.isnull().sum()/len(train)*100\n","# print(train.head())\n","\n","# As you can see in the above table, there aren’t too many missing values (just 2 variables have them actually). We can impute the values using appropriate methods, or we can set a threshold of, say 20%, and remove the variable having more than 20% missing values. Let’s look at how this can be done in Python:\n","\n","# saving missing values in a variable\n","a = train.isnull().sum()/len(train)*100\n","print(a)\n","\n","# saving column names in a variable\n","variables = train.columns\n","print(variables)\n","variable = [ ]\n","for i in range(0,12):\n","    if a[i]<=20:   #setting the threshold as 20%\n","        variable.append(variables[i])\n","\n","#So the variables to be used are stored in “variable”, which contains only those features where the missing values are less than 20%.\n","print (variable)\n","#Let us drop the Outlet_Size column\n","\n","train = train.drop(columns=['Outlet_Size'])\n","\n","#Let’s first impute the missing values in the Item_Weight column using the median value of the known Item_Weight observations.\n","\n","train['Item_Weight'].fillna(train['Item_Weight'].median(), inplace=True)\n","\n","train.isnull().sum()/len(train)*100\n"]}]}